@article{Eberhardt2013,
   abstract = {A large body of literature estimates private returns to R&D adopting the Griliches knowledge production framework, which ignores the potential impact of spillovers on consistent estimation. Using a panel of twelve manufacturing industries across ten OECD economies, we investigate whether ignoring spillovers leads to bias in the estimated private returns to R&D. We compare results from a common factor framework, which accounts for spillovers and other unobserved shocks, to those from a standard Griliches approach. Our findings confirm that conventional estimates conflate own-R&D and spillover effects, implying that spillovers cannot be ignored even when the interest lies exclusively in evaluating private returns to R&D. © 2013 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology.},
   author = {Markus Eberhardt and Christian Helmers and Hubert Strauss},
   doi = {10.1162/REST_a_00272},
   issn = {00346535},
   issue = {2},
   journal = {Review of Economics and Statistics},
   pages = {436-448},
   title = {Do spillovers matter when estimating private returns to R&D?},
   volume = {95},
   year = {2013},
}
@article{Conley2010,
   author = {Timothy G Conley and Christopher R Udry},
   doi = {10.1257/aer},
   issue = {1},
   journal = {American Economic Review},
   month = {3},
   pages = {35-69},
   title = {Learning about a New Technology: Pineapple in Ghana},
   volume = {100},
   year = {2010},
}
@article{Cao2021,
   abstract = {This paper presents and analyzes an approach to cluster-based inference for dependent data. The primary setting considered here is with spatially indexed data in which the dependence structure of observed random variables is characterized by a known, observed dissimilarity measure over spatial indices. Observations are partitioned into clusters with the use of an unsupervised clustering algorithm applied to the dissimilarity measure. Once the partition into clusters is learned, a cluster-based inference procedure is applied to a statistical hypothesis testing procedure. The procedure proposed in the paper allows the number of clusters to depend on the data, which gives researchers a principled method for choosing an appropriate clustering level. The paper gives conditions under which the proposed procedure asymptotically attains correct size. A simulation study shows that the proposed procedure attains near nominal size in finite samples in a variety of statistical testing problems with dependent data.},
   author = {Jianfei Cao and Christian Hansen and Damian Kozbur and Lucciano Villacorta},
   month = {7},
   title = {Inference for Dependent Data with Learned Clusters},
   url = {http://arxiv.org/abs/2107.14677},
   year = {2021},
}
@article{Conley2018,
   abstract = {We review developments in conducting inference for model parameters in the presence of intertemporal and cross-sectional dependence with an emphasis on panel data applications. We review the use of heteroskedasticity and autocorrelation consistent (HAC) standard error estimators, which include the standard clustered and multiway clustered estimators, and discuss alternative sample-splitting inference procedures, such as the Fama–Macbeth procedure, within this context. We outline pros and cons of the different procedures. We then illustrate the properties of the discussed procedures within a simulation experiment designed to mimic the type of firm-level panel data that might be encountered in accounting and finance applications. Our conclusion, based on theoretical properties and simulation performance, is that sample-splitting procedures with suitably chosen splits are the most likely to deliver robust inferential statements with approximately correct coverage properties in the types of large, heterogeneous panels many researchers are likely to face.},
   author = {Timothy Conley and Silvia Gonçalves and Christian Hansen},
   doi = {10.1111/1475-679X.12219},
   issn = {1475679X},
   issue = {4},
   journal = {Journal of Accounting Research},
   keywords = {bootstrap,confidence intervals,fixed-effects,hypothesis testing,robust standard error estimation,spatial dependence},
   month = {9},
   pages = {1139-1203},
   publisher = {Blackwell Publishing Ltd},
   title = {Inference with Dependent Data in Accounting and Finance Applications},
   volume = {56},
   year = {2018},
}
@article{Conleyetal2003,
   abstract = {This paper examines whether spillovers from local market human capital are important in explaining the distribution of productivity across Malaysia. We develop an empirical method for describing local human capital distributions based on the idea that spillovers are limited in scope by costs of interaction or economic distance between agents. We use estimates of the economic distance between agents to construct measures of local market human capital based on schooling rates of the population within a given radius. These measures are then used in estimating equations obtained from a simple local public goods model. Our regressions are estimated using spatial GMM, allowing for general spatial correlation across observations as a function of economic distance. We find positive wage and rent differentials associated with local human capital, evidence consistent with productive human capital spillovers. Our results for rent differentials obtain with two distinct human capital measures; however, those for wage differentials depend on the human capital measure used.},
   author = {Timothy G Conley and Fredrick Flyer and Grace R Tsiang},
   issue = {1},
   journal = {Advances in Economic Analysis and Policy3},
   keywords = {Human Capital Spillovers,Local Public Goods,Spatial GMM},
   title = {Spillovers from Local Market Human Capital and the Spatial Distribution of Productivity in Malaysia},
   volume = {3},
   url = {http://www.bepress.com/bejeap.},
   year = {2003},
}
@misc{Conley2003,
   abstract = {This paper presents a spatial econometric method for characterizing productivity comovement across sectors of the U.S. economy. Input-output relations provide an economic distance measure that is used to characterize interactions between sectors, as well as conduct estimation and inference. We construct two different economic distance measures. One metric implies that two sectors are close to one another if they use inputs of other industrial sectors in nearly the same proportion , and the other metric implies that sectors are close if their outputs are used by the same sectors. Our model holds that covariance in productivity growth across sectors is a function of economic distance. We find that (1) positive cross-sector covariance of productivity growth generates a substantial fraction of the variance in aggregate productivity, (2) cross-sector productivity covariance tends to be greatest between sectors with similar input relations, and (3) there are constant to modest increasing returns to scale. We test and reject the hypothesis that these correlations are due to a common shock. We thank Craig Burnside, Martin Eichenbaum, and Sergio Rebelo for sharing with us quarterly manufacturing data. We thank},
   author = {Timothy G Conley and Bill Dupor and Gadi Barlevy and Susanto Basu and Gerald Carlino and Xiao-Hong Chen and Adeline Delavande and Martin Eichenbaum and John Fernald and Joshua Hojvat-Gallin and Michael Horvath and Beth Ingram and Andreas Lehnert and Steven Levitt and Sergio Rebelo and John Shea and Richard Spady and Dan Wilson},
   issue = {2},
   journal = {Journal of Political Economy},
   title = {A Spatial Analysis of Sectoral Complementarity},
   volume = {111},
   year = {2003},
}
@article{Conley1999,
   abstract = {This paper presents a spatial model of dependence among agents using a metric of economic distance. Measurements of this economic distance provide cross-sectional data with a structure similar to that provided by the time index in time-series data. Generalized method of moments estimators using such dependent data are shown to be consistent and asymptotically normal. This paper presents a class of non-parametric, positive semi-de"nite covariance matrix estimators that allow for general forms of dependence characterized by economic distance. These covariance matrix estimators are shown to remain consistent when economic distances are not precisely observed. 1999 Elsevier Science S.A. All rights reserved.},
   author = {T G Conley},
   journal = {Journal of Econometrics},
   keywords = {C14 Keywords: Cross-sectional dependence,Generalized method of moments,JEL classixcation: C50,Non-parametric covariance matrix estimation,Random "elds},
   pages = {1-45},
   title = {GMM estimation with cross sectional dependence},
   volume = {92},
   year = {1999},
}
@article{Bester2011,
   abstract = {This paper presents an inference approach for dependent data in time series, spatial, and panel data applications. The method involves constructing t and Wald statistics using a cluster covariance matrix estimator (CCE). We use an approximation that takes the number of clusters/groups as fixed and the number of observations per group to be large. The resulting limiting distributions of the t and Wald statistics are standard t and F distributions where the number of groups plays the role of sample size. Using a small number of groups is analogous to 'fixed-b' asymptotics of Kiefer and Vogelsang (2002, 2005) (KV) for heteroskedasticity and autocorrelation consistent inference. We provide simulation evidence that demonstrates that the procedure substantially outperforms conventional inference procedures. © 2011 Elsevier B.V. All rights reserved.},
   author = {C. Alan Bester and Timothy G. Conley and Christian B. Hansen},
   doi = {10.1016/j.jeconom.2011.01.007},
   issn = {03044076},
   issue = {2},
   journal = {Journal of Econometrics},
   keywords = {HAC,Panel,Robust,Spatial},
   month = {12},
   pages = {137-151},
   title = {Inference with dependent data using cluster covariance estimators},
   volume = {165},
   year = {2011},
}
@article{Conley2018,
   abstract = {We review developments in conducting inference for model parameters in the presence of intertemporal and cross-sectional dependence with an emphasis on panel data applications. We review the use of heteroskedasticity and autocorrelation consistent (HAC) standard error estimators, which include the standard clustered and multiway clustered estimators, and discuss alternative sample-splitting inference procedures, such as the Fama–Macbeth procedure, within this context. We outline pros and cons of the different procedures. We then illustrate the properties of the discussed procedures within a simulation experiment designed to mimic the type of firm-level panel data that might be encountered in accounting and finance applications. Our conclusion, based on theoretical properties and simulation performance, is that sample-splitting procedures with suitably chosen splits are the most likely to deliver robust inferential statements with approximately correct coverage properties in the types of large, heterogeneous panels many researchers are likely to face.},
   author = {Timothy Conley and Silvia Gonçalves and Christian Hansen},
   doi = {10.1111/1475-679X.12219},
   issn = {1475679X},
   issue = {4},
   journal = {Journal of Accounting Research},
   keywords = {bootstrap,confidence intervals,fixed-effects,hypothesis testing,robust standard error estimation,spatial dependence},
   month = {9},
   pages = {1139-1203},
   publisher = {Blackwell Publishing Ltd},
   title = {Inference with Dependent Data in Accounting and Finance Applications},
   volume = {56},
   year = {2018},
}
@article{Muller2022JBES,
   abstract = {We consider inference about a scalar coefficient in a linear regression with spatially correlated errors. Recent suggestions for more robust inference require stationarity of both regressors and dependent variables for their large sample validity. This rules out many empirically relevant applications, such as difference-indifference designs. We develop a robustified version of the recently suggested SCPC method that addresses this challenge. We find that the method has good size properties in a wide range of Monte Carlo designs that are calibrated to real world applications, both in a pure cross sectional setting, but also for spatially correlated panel data. We provide numerically efficient methods for computing the associated spatial-correlation robust test statistics, critical values, and confidence intervals. ARTICLE HISTORY},
   author = {Ulrich K Müller and Mark W Watson},
   doi = {10.1080/07350015.2022.2127737},
   issue = {0},
   journal = {Journal of Business & Economic Statistics},
   keywords = {HAC,HAR,Matérn process},
   pages = {1-15},
   title = {Spatial Correlation Robust Inference in Linear Regression and Panel Models},
   volume = {00},
   url = {https://www.tandfonline.com/action/journalInformation?journalCode=ubes20},
   year = {2022},
}
@article{Muller2022ECTA,
   abstract = {We propose a method for constructing confidence intervals that account for many forms of spatial correlation. The interval has the familiar "estimator plus and minus a standard error times a critical value" form, but we propose new methods for constructing the standard error and the critical value. The standard error is constructed using population principal components from a given "worst-case" spatial correlation model. The critical value is chosen to ensure coverage in a benchmark parametric model for the spatial correlations. The method is shown to control coverage in finite sample Gaussian settings in a restricted but nonparametric class of models and in large samples whenever the spatial correlation is weak, that is, with average pairwise correlations that vanish as the sample size gets large. We also provide results on the efficiency of the method.},
   author = {Ulrich K Müller and Mark W Watson},
   doi = {10.3982/ECTA19465},
   issue = {6},
   journal = {Econometrica},
   keywords = {Confidence interval,HAC,HAR,random field},
   pages = {2901-2935},
   title = {SPATIAL CORRELATION ROBUST INFERENCE},
   volume = {90},
   url = {https://doi.org/10.3982/ECTA19465},
   year = {2022},
}
@article{Gandhi2020,
   abstract = {We study the nonparametric identification of gross output production functions under the environment of the commonly employed proxy variable methods. We show that applying these methods to gross output requires additional sources of variation in the demand for flexible inputs (e.g., prices). Using a transformation of the firm's first-order condition, we develop a new nonparametric identification strategy for gross output that can be employed even when additional sources of variation are not available. Monte Carlo evidence and estimates from Colombian and Chilean plant-level data show that our strategy performs well and is robust to deviations from the baseline setting.},
   author = {Amit Gandhi and Salvador Navarro and David A Rivers},
   issue = {8},
   journal = {Journal of Political Economy},
   month = {6},
   pages = {2973-3016},
   title = {On the Identification of Gross Output Production Functions},
   volume = {128},
   year = {2020},
}

